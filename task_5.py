# -*- coding: utf-8 -*-
"""Task 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B8IEQ2NKGljfwjcOHESN3a-SgYAvoljr
"""

pip install graphviz

# Decision Tree & Random Forest

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import graphviz

# 1. Load and Preprocess Data
df = pd.read_csv("titanic.csv")

# Select useful columns
features = ["Pclass", "Sex", "Age", "SibSp", "Parch", "Fare"]
df = df[["Survived"] + features]

# Handle missing values
df["Age"].fillna(df["Age"].median(), inplace=True)
df["Fare"].fillna(df["Fare"].median(), inplace=True)

X = df[features]
y = df["Survived"]

numeric_features = ["Age", "SibSp", "Parch", "Fare"]
categorical_features = ["Sex", "Pclass"]

# Preprocessing (important for industry)
numeric_transformer = Pipeline(steps=[
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("encoder", OneHotEncoder(drop="first"))
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ]
)
# 2. Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 3. Decision Tree Classifier

dt_model = Pipeline(steps=[
    ("prep", preprocess),
    ("clf", DecisionTreeClassifier(max_depth=5, random_state=42))
])

dt_model.fit(X_train, y_train)

y_pred_dt = dt_model.predict(X_test)
dt_acc = accuracy_score(y_test, y_pred_dt)

print("Decision Tree Accuracy:", dt_acc)
print("\nDecision Tree Report:\n", classification_report(y_test, y_pred_dt))


# 4. Visualize the Decision Tree

feature_names = (
    numeric_features +
    list(preprocess.named_transformers_["cat"]
         .named_steps["encoder"]
         .get_feature_names_out(categorical_features))
)

tree_clf = dt_model.named_steps["clf"]

dot = export_graphviz(
    tree_clf,
    out_file=None,
    feature_names=feature_names,
    class_names=["Died", "Survived"],
    filled=True,
    rounded=True
)

graphviz.Source(dot)


# 5. Overfitting Control: Accuracy vs Tree Depth

depths = range(1, 12)
train_scores = []
test_scores = []

for d in depths:
    model = Pipeline(steps=[
        ("prep", preprocess),
        ("clf", DecisionTreeClassifier(max_depth=d, random_state=42))
    ])
    model.fit(X_train, y_train)
    train_scores.append(model.score(X_train, y_train))
    test_scores.append(model.score(X_test, y_test))

plt.figure(figsize=(8,5))
plt.plot(depths, train_scores, label="Train Accuracy")
plt.plot(depths, test_scores, label="Test Accuracy")
plt.xlabel("Tree Depth")
plt.ylabel("Accuracy")
plt.title("Decision Tree Overfitting Analysis")
plt.legend()
plt.show()

# 6. Random Forest Classifier
rf_model = Pipeline(steps=[
    ("prep", preprocess),
    ("clf", RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        random_state=42
    ))
])

rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)
rf_acc = accuracy_score(y_test, y_pred_rf)

print("\nRandom Forest Accuracy:", rf_acc)
print("\nRandom Forest Report:\n", classification_report(y_test, y_pred_rf))

# ------------------------------------------------------------
# 7. Feature Importances (Random Forest)
# ------------------------------------------------------------
rf_clf = rf_model.named_steps["clf"]
importances = rf_clf.feature_importances_

feat_imp = pd.DataFrame({
    "Feature": feature_names,
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

print("\nFeature Importances:\n", feat_imp)

plt.figure(figsize=(8,5))
sns.barplot(data=feat_imp, x="Importance", y="Feature")
plt.title("Random Forest Feature Importance")
plt.show()


# 8. Cross-Validation (5-Fold)

cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring="accuracy")
print("\nCross Validation Scores:", cv_scores)
print("Mean CV Accuracy:", cv_scores.mean())

